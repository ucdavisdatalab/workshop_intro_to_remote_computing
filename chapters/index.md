Overview
========

This collection of workshops provides an introduction to remote computing. The
collection has two parts:

* **Overview of Remote and High-Performance Computing** (one 2-hour session):
  Are you working on a research project but finding that your data is too big
  for your laptop, or that your code must run for a long time? You need more
  compute power! During this session we’ll discuss the differences and
  advantages of various remote and networked computing options, from servers in
  your lab to institutional high performance computing (HPC) and cloud
  services. We’ll cover an overview of HPC terminology, architecture, and
  general workflows. We’ll also provide information about UC-specific computing
  resources and contacts. This workshop is a prerequisite introduction to
  DataLab’s Introduction to Remote Computing series where you’ll learn how to
  access and work efficiently on the UC Davis HPC.

  :::{admonition} Learning Goals
  :class: note, dropdown
  After completing this workshop, learners should be able to:

  + Compare local, remote, HPC, and cloud computing options
  + Define common compute jargon including cluster, supercomputer, nodes,
  + Identify the skills needed for working on remote and HPC systems
  + Describe how remote and high-performance computing could benefit their
    research
  + Identify where to go to learn more!
  :::

  :::{important}
  [This slide deck][slides] is the only material for this workshop.

  [slides]: https://docs.google.com/presentation/d/1Y14U4okdZ9MsdYJ997cCQexQoyiDfv0NrDfo49pr2nQ/edit?usp=sharing
  :::

* **Introduction to Remote Computing** (four 2-hour sessions): This workshop
  series provides an introduction to accessing and computing on remote servers
  such as [UC Davis' "Hive" cluster][hive]. The series covers everything you
  need to know to get started: how to set up and use SSH to log in and transfer
  files, how to install software with conda, how to reserve computing time and
  run programs with SLURM, and shell commands that are especially useful for
  working with servers.

  [hive]: https://hpc.ucdavis.edu/clusters

  :::{admonition} Learning Goals
  :class: note, dropdown
  After completing this workshop series, learners should be able to:
  
  + Use SSH to log in to a server
  + Transfer files to and from a server
  + Set up and use conda/mamba to install software on a server
  + Use SLURM to run interactive and non-interactive software on a server
  + Explain etiquette for using a server or compute cluster
  :::

  :::{admonition} Prerequisites
  :class: caution
  Participants must have taken DataLab’s "Overview of Remote and High
  Performance Computing (HPC)” workshop and "Introduction to the Command Line"
  workshop series, or have equivalent prior experience. Participants must be
  comfortable with basic Linux shell syntax.
  :::
